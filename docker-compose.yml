services:
  backend:
    build:
      context: ./Backend
    ports:
      - "8000:8000"
    env_file:
      - .env
    depends_on:
      - ml-api
      - postgres
    networks:
      - app-network

  ml-worker:
    build:
      context: ./ML
    command: celery -A app.celery_worker.celery_app worker --loglevel=info -c 1
    env_file:
      - .env
    volumes:
      - ml-rag-db:/app/db
    depends_on:
      - redis
      - ollama
    networks:
      - app-network

  redis:
    image: redis:8-alpine
    networks:
      - app-network

  ml-api:
    build:
      context: ./ML
      dockerfile: Dockerfile
    volumes:
      - ml-rag-db:/app/db
    ports:
      - "8001:8001"
    env_file:
      - .env
    depends_on:
      - ollama
      - redis
    networks:
      - app-network

  ollama:
    build:
      context: ./ML
      dockerfile: Dockerfile.ollama
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    # если есть gpu - раскомментировать
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    networks:
      - app-network

  postgres:
    image: postgres:17
    volumes:
      - postgres-data:/var/lib/postgresql/data
    env_file:
      - .env
    ports:
      - "5432:5432"
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  ollama-data:
  postgres-data:
  ml-rag-db:
